{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9426131,"sourceType":"datasetVersion","datasetId":5725956}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Crimes to be covered \n# -Shooting\n# -Arson\n# -Vandalism","metadata":{"execution":{"iopub.status.busy":"2024-10-09T07:08:55.731757Z","iopub.execute_input":"2024-10-09T07:08:55.732192Z","iopub.status.idle":"2024-10-09T07:08:56.178474Z","shell.execute_reply.started":"2024-10-09T07:08:55.732151Z","shell.execute_reply":"2024-10-09T07:08:56.176596Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-09T07:08:56.179533Z","iopub.status.idle":"2024-10-09T07:08:56.179996Z","shell.execute_reply.started":"2024-10-09T07:08:56.179775Z","shell.execute_reply":"2024-10-09T07:08:56.179796Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Requirements:\ntorch\ntorchvision\nnumpy\nopenai-clip\ntqdm\nPillow\ntransformers","metadata":{}},{"cell_type":"code","source":"# import json\n# from PIL import Image\n# import os\n# import shutil\n\n# from tqdm import tqdm\n\n# import torch\n# import torch.nn as nn\n# from torch.utils.data import DataLoader\n\n# import clip\n# from transformers import CLIPProcessor, CLIPModel\n\n# # Paths to data\n# image_folder = \"/home/server_admin/Desktop/indo_clip/data_full/images/\"\n# caption_folder = \"/home/server_admin/Desktop/indo_clip/data_full/text/\"\n\n# def get_image_caption_paths(image_folder, caption_folder):\n#     # Initialize lists to store image paths and caption paths\n#     image_paths = []\n#     caption_paths = []\n    \n#     # Get list of filenames in the image folder\n#     image_filenames = os.listdir(image_folder)\n    \n#     # Iterate over each filename\n#     for filename in image_filenames:\n#         # Construct full path for image and caption\n#         image_path = os.path.join(image_folder, filename)\n#         caption_filename = filename.replace(\".jpg\", \".txt\")  # Assuming captions are in .txt format\n#         caption_path = os.path.join(caption_folder, caption_filename)\n        \n#         # Check if both image and corresponding caption exist\n#         if os.path.exists(image_path) and os.path.exists(caption_path):\n#             # Append paths to the lists\n#             image_paths.append(image_path)\n#             caption_paths.append(caption_path)\n    \n#     return image_paths, caption_paths\n \n# def read_first_line(file_path):\n#     with open(file_path, 'r') as file:\n#         first_line = file.readline()\n#     return first_line   \n\n# # Load the CLIP model and processor\n# #model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n# #processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# # Choose computation device\n# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" \n\n# # Load pre-trained CLIP model\n# model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n# #processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n# # ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14']\n\n# # Get images and their captions\n# image_paths, caption_paths = get_image_caption_paths(image_folder, caption_folder)\n        \n\n# # Define a custom dataset\n# class image_title_dataset():\n#     def __init__(self, list_image_path,list_txt):\n#         # Initialize image paths and corresponding texts\n#         self.image_path = list_image_path\n#         # Tokenize text using CLIP's tokenizer\n#         self.title = list_txt\n#         #self.title  = clip.tokenize(list_txt)\n    \n#     def __len__(self):\n#         return len(self.title)\n\n#     def __getitem__(self, idx):\n#         # Preprocess image using CLIP's preprocessing function\n#         image = preprocess(Image.open(self.image_path[idx])) # (3,224,224)\n#         text = read_first_line(self.title[idx]) \n#         #text = clip.tokenize(text) # (1,77)\n#         #title = self.title\n#         #print(title.shape)\n#         return image, text\n\n# # Create datasets\n# train_dataset = image_title_dataset(image_paths, caption_paths)\n\n# # Create data loaders\n# batch_size = 256\n# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# # Function to convert model's parameters to FP32 format\n# def convert_models_to_fp32(model): \n#     for p in model.parameters(): \n#         p.data = p.data.float() \n#         p.grad.data = p.grad.data.float() \n\n\n# if device == \"cpu\":\n#   model.float()\n\n# # Prepare the optimizer\n# optimizer = torch.optim.Adam(model.parameters(), lr=10e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) # the lr is smaller, more safe for fine tuning to new dataset\n\n# # Define loss function\n# loss_img = nn.CrossEntropyLoss()\n# loss_txt = nn.CrossEntropyLoss()\n\n# # Train the model\n# num_epochs = 50\n# for epoch in range(num_epochs):\n#     pbar = tqdm(train_loader, total=len(train_loader))\n#     for batch in pbar:\n#         optimizer.zero_grad()\n\n#         images, texts = batch \n#         images = images.to(device)\n#         texts = clip.tokenize(texts).to(device)\n\n#         # Forward pass\n#         logits_per_image, logits_per_text = model(images, texts)\n\n#         # Compute loss\n#         ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n#         total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n\n#         # Backward pass\n#         total_loss.backward()\n#         if device != \"cpu\":\n#             clip.model.convert_weights(model)\n#         optimizer.step()\n\n#         pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n\n# # Save the trained model\n# output_dir = \"saved_models\"\n# os.makedirs(output_dir, exist_ok=True)\n# model_save_path = os.path.join(output_dir, \"clip_full_50_VT32.pt\")\n# torch.save(model.state_dict(), model_save_path)\n# print(f\"Trained model saved at {model_save_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-09T07:08:56.181973Z","iopub.status.idle":"2024-10-09T07:08:56.182650Z","shell.execute_reply.started":"2024-10-09T07:08:56.182325Z","shell.execute_reply":"2024-10-09T07:08:56.182360Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install git+https://github.com/openai/CLIP.git","metadata":{"execution":{"iopub.status.busy":"2024-10-09T08:21:49.833841Z","iopub.execute_input":"2024-10-09T08:21:49.834303Z","iopub.status.idle":"2024-10-09T08:22:08.694221Z","shell.execute_reply.started":"2024-10-09T08:21:49.834261Z","shell.execute_reply":"2024-10-09T08:22:08.692743Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom PIL import Image\nimport os\nimport shutil\nimport cv2\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport clip\nfrom transformers import CLIPProcessor, CLIPModel\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# # Paths to data\n# image_folder = \"/home/server_admin/Desktop/indo_clip/data_full/images/\"\n# caption_folder = \"/home/server_admin/Desktop/indo_clip/data_full/text/\"\n\n# def get_image_caption_paths(image_folder, caption_folder):\n#     # Initialize lists to store image paths and caption paths\n#     image_paths = []\n#     caption_paths = []\n    \n#     # Get list of filenames in the image folder\n#     image_filenames = os.listdir(image_folder)\n    \n#     # Iterate over each filename\n#     for filename in image_filenames:\n#         # Construct full path for image and caption\n#         image_path = os.path.join(image_folder, filename)\n#         caption_filename = filename.replace(\".jpg\", \".txt\")  # Assuming captions are in .txt format\n#         caption_path = os.path.join(caption_folder, caption_filename)\n        \n#         # Check if both image and corresponding caption exist\n#         if os.path.exists(image_path) and os.path.exists(caption_path):\n#             # Append paths to the lists\n#             image_paths.append(image_path)\n#             caption_paths.append(caption_path)\n    \n#     return image_paths, caption_paths\n \n# def read_first_line(file_path):\n#     with open(file_path, 'r') as file:\n#         first_line = file.readline()\n#     return first_line   \n\n# # Load the CLIP model and processor\n# #model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n# #processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# # Choose computation device\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" \n\n# # Load pre-trained CLIP model\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n# # ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14']\n\n# # Get images and their captions\n# image_paths, caption_paths = get_image_caption_paths(image_folder, caption_folder)\n        \n\n# # Define a custom dataset\n# class image_title_dataset():\n#     def __init__(self, list_image_path,list_txt):\n#         # Initialize image paths and corresponding texts\n#         self.image_path = list_image_path\n#         # Tokenize text using CLIP's tokenizer\n#         self.title = list_txt\n#         #self.title  = clip.tokenize(list_txt)\n    \n#     def __len__(self):\n#         return len(self.title)\n\n#     def __getitem__(self, idx):\n#         # Preprocess image using CLIP's preprocessing function\n#         image = preprocess(Image.open(self.image_path[idx])) # (3,224,224)\n#         text = read_first_line(self.title[idx]) \n#         #text = clip.tokenize(text) # (1,77)\n#         #title = self.title\n#         #print(title.shape)\n#         return image, text\n\n# # Create datasets\n# train_dataset = image_title_dataset(image_paths, caption_paths)\n\n# Create data loaders\n\ntrain_file_json = open(\"/kaggle/input/ucaucf-crime-annotation-dataset/UCFCrime_Train.json\");\ntrain_file = json.load(train_file_json);\n\ntrain_dataset = []\n\n\n\nprocess_whole_dataset = False ## CHANGE THIS\n\n\n\nfor video_file_name in train_file:\n    category = video_file_name.split('_')[0][0:-3]\n    if (category not in ['Shooting', 'Arson', 'Vandalism']): #CATEGORIES HARD CODED HERE, CHANGE IF NEEDED!!\n        continue\n    yes = True\n    video_path = \"/kaggle/input/ucaucf-crime-annotation-dataset/UCF_Crimes/UCF_Crimes/Videos/\" + category + \"/\" + video_file_name + \".mp4\"\n    cap = cv2.VideoCapture(video_path)\n    if (cap.isOpened()== False):\n        print(\"Error opening video file\")\n    video = []\n    while(cap.isOpened()):\n        ret, frame = cap.read()\n        if ret == True:\n            frame = Image.fromarray(np.uint8(frame)).convert('RGB')\n            frame = preprocess(frame)\n            video.append(frame)\n        else:\n            break\n    cap.release()\n    timestamps_list_iterator = 0\n    for timestamps_list in train_file[video_file_name]['timestamps']: \n        timestamps = [x / 10 for x in range(int(timestamps_list[0]*10), int(timestamps_list[1]*10))]\n        video_length = train_file[video_file_name]['duration']\n        timestamps_indexes = [int((x / video_length) * len(video)) for x in timestamps]\n        for timestamp_index in timestamps_indexes:\n            train_dataset.append((video[timestamp_index], train_file[video_file_name]['sentences'][timestamps_list_iterator]))\n        timestamps_list_iterator += 1\n    if yes: \n        break\n        \nbatch_size = 256\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# Function to convert model's parameters to FP32 format\ndef convert_models_to_fp32(model): \n    for p in model.parameters(): \n        p.data = p.data.float() \n        p.grad.data = p.grad.data.float() \n\n\nif device == \"cpu\":\n  model.float()\n\n# Prepare the optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=10e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) # the lr is smaller, more safe for fine tuning to new dataset\n\n# Define loss function\nloss_img = nn.CrossEntropyLoss()\nloss_txt = nn.CrossEntropyLoss()\n\n# Train the model\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    pbar = tqdm(train_loader, total=len(train_loader))\n    for batch in pbar:\n        optimizer.zero_grad()\n\n        images, texts = batch \n        images = images.to(device)\n        texts = clip.tokenize(texts).to(device)\n\n        # Forward pass\n        logits_per_image, logits_per_text = model(images, texts)\n\n        # Compute loss\n        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n        total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n\n        # Backward pass\n        total_loss.backward()\n        if device != \"cpu\":\n            clip.model.convert_weights(model)\n        optimizer.step()\n\n        pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n\n# Save the trained model\noutput_dir = \"saved_models\"\nos.makedirs(output_dir, exist_ok=True)\nmodel_save_path = os.path.join(output_dir, \"clip_full_50_VT32_50.pt\")\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Trained model saved at {model_save_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}